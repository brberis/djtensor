#version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    command: /bin/sh -c "sleep 5 && 
                    python manage.py makemigrations --noinput &&
                    python manage.py migrate --noinput &&
                    python manage.py shell -c \"from django.contrib.auth.models import User; User.objects.filter(username='testuser').exists() or User.objects.create_superuser('testuser', 'testuser@example.com', 'testpassword')\" &&
                    python manage.py runserver 0.0.0.0:8000" 
    volumes:
      - static_volume:/usr/src/app/staticfiles
      - media_volume:/usr/src/app/mediafiles
      - ./backend/:/usr/src/app/
    expose:
      - 8000
    env_file:
      - ./.env.prod
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
    depends_on:
      - db
      - redis
    networks:
      - backend

  db:
    image: postgres:15
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=backend
      - POSTGRES_PASSWORD=backend
      - POSTGRES_DB=backend_prod
    networks:
      - backend
      
  redis:
    image: redis
    ports:
      - 6379:6379
    networks:
      - backend

  celery_worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.prod    
    command: celery -A celery_app worker --loglevel=info --max-tasks-per-child=1
    env_file:
      - ./.env.prod
    depends_on:
      - db    
      - redis
    environment:
      - CELERYD_MAX_TASKS_PER_CHILD=1
      - CELERYD_TASK_TIME_LIMIT=3600 
      - CELERYD_TASK_SOFT_TIME_LIMIT=3500
      - LD_LIBRARY_PATH="$CUDNN_PATH/lib":"/usr/local/cuda/lib64"
      - NVIDIA_VISIBLE_DEVICES=all
      - TF_FORCE_GPU_ALLOW_GROWTH=true
      - PATH="$PATH":"/usr/local/cuda/bin"
      - PATH=/usr/local/cuda/bin:$PATH
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
      - TF_GPU_ALLOCATOR=cuda_malloc_async

    volumes:
      - ./backend/:/usr/src/app/
      - static_volume:/usr/src/app/staticfiles  
      - media_volume:/usr/src/app/mediafiles 
    deploy:
      resources:
        limits:
          memory: 8g
        reservations:
          memory: 4g
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    runtime: nvidia
    ipc: host
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864
    networks:
      - backend
    restart: always

  frontend:
    build: ./frontend
    command: npm run start
    volumes:
      - static_volume:/usr/src/app/.next/static
    expose:
      - 3000
    env_file:
      - ./.env.prod
    networks:
      - backend

  nginx:
    build: ./nginx
    volumes:
      - static_volume:/usr/src/app/staticfiles
      - media_volume:/usr/src/app/mediafiles
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - backend
    networks:
      - backend

volumes:
  postgres_data:
  static_volume:
  media_volume:

networks:
  backend:
    driver: bridge
